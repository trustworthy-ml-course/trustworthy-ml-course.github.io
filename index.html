<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>Trustworthy Artificial Intelligence Course: Professor Birhanu Eshete</title>
    <!-- Bootstrap CSS -->
	<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
</head>
<body>


    <!-- Main Content -->
    
<div class="container mt-4">

<table class="table table-bordered">
            <thead style="background-color:#00274C;color:#FFCB05" align ="center" border="0">
                <tr id="section1">
                    <th align="center">
			    <h1>CIS 482/582: Trustworthy Artificial Intelligence</h1>
			    <h2>University of Michigan, Dearborn</h2>
			    <h2>Instructor: Prof. Birhanu Eshete</h2>
		    </th>
			
                    
                </tr>
	    </thead>
</table>
	<div class="alert alert-primary" role="alert" style="background-color:#FFCB05;color:#00274C">
		<b>Course Information:</b>
		<ul>
			<li><b>Current Offering Term</b>: Winter 2024, University of Michigan, Dearborn</li>
			<li><b>Instructor</b>: <a href ="https://www-personal.umd.umich.edu/~birhanu/" target="_blank">Prof. Birhanu Eshete</a>: <a href="mailto:birhanu@umich.edu">birhanu@umich.edu</a>; Office: CIS 229</li>
			<li><b>Teaching Assistant(s)</b>: Abe Amich: <a href="mailto:aamich@umich.edu">aamich@umich.edu</a></li>
			<li><b>Time</b>: Mondays 6pm - 8:45pm</li>
			<li><b>Venue</b>: HPEC 1181</li>
			<li><b>Office hours</b>: Tuesdays 3pm - 4:30pm or by (virtual/in-person) appointment</li>
			<li><b>Canvas</b>: If you are a UMICH-Dearborn student enrolled in this course, <a href="https://canvas.umd.umich.edu/courses/537448" target="_blank">access specifics here with umich credentials</a></li>
		</ul>
	</div>

		<div class="alert alert-primary" role="alert" style="background-color:#FFCB05;color:#00274C">

            		<p><b>Course Description</b>: 
This course introduces students to the broad and emerging notion of trustworthy artificial intelligence (AI). Beginning with a hands-on introduction to the basics of Deep Neural Networks (DNNs) and modeling, it will cover three broad areas of trustworthiness in AI. In the first area of robustness, the course will introduce students to the AI threat landscape focusing on training data poisoning, model evasion, privacy-sensitive data inference, model stealing/extraction, and threats to safe deployment of AI.  In the second area of transparency, students will be introduced to frameworks used to interpret/explain AI model’s decisions.   In the third area of accountability, students will learn methods and tools for reducing bias and ethical pitfalls when AI models are deployed in high-stakes application domains. The course concludes with a broader take on AI trustworthiness by studying the dynamics among the three broad AI trustworthiness desirables. The course will be taught in a predominantly project-based setting to allow students gain hands-on experience beyond conceptual understanding.</p>

		</div>

	<div class="alert alert-primary" role="alert" style="background-color:#FFCB05;color:#00274C">

            		<p><b>On Prerequisites</b>: While prior knowledge of machine learning is not required, it will be a plus. To level the ground for everyone, the course will kick-off with a ML crash course just enough to understand subsequent material. Students are expected to have proficiency in at least one programming language (e.g., Python, C/C++, Java). Knowledge of data structures such as trees and graphs would be a plus.</p>
		</div>

		<div class="alert alert-primary" role="alert">

			<p>	<b>Reference Materials</b>: This course doesn’t have a dedicated textbook. However, we will use the following three books as our main references. In addition to these books, the course will heavily rely on influential papers for each topic discussed.</p>

			<ol>
		
				<li>Trustworthy Machine Learning by Kush R. Varshney, Independently Published, 2022: <a href= "http://www.trustworthymachinelearning.com/trustworthymachinelearning.pdf">here</a></li>
	
				<li>Adversarial Machine Learning by Joseph, Nelson, Rubinstein, and Tygar: <a href="https://www.cambridge.org/core/books/adversarial-machine-learning/C42A9D49CBC626DF7B8E54E72974AA3B">here</a></li>
				<li>Fairness in Machine Learning: Limitations and Opportunities by Solon Barocas, Moritz Hardt, Arvind Narayanan: <a href="https://fairmlbook.org/">here</a></li>
			</ol>
		</div>

		<div class="alert alert-danger" role="alert" style="background-color:#FFCB05;color:#00274C">
			<p>
			<b>On Scope</b>: While this course is about AI/ML, it does not cover formalisms or technical details of ML or Deep Neural Networks. Deep learning fundamentals just enough to grasp subsequent topics are introduced at the beginning of the course. This course is intentionally broad so as to reason about ML trustworthiness beyond ML in the presence of adversaries. It is organized in a manner that expands the focus beyond ML security and privacy to safety, transparency, fairness, and ethical implications of AI/ML deployed in high-stakes application domains. Given the natural focus on breadth instead of depth, emphasis is more on representative trustworthiness risks/pitfalls and remedies/best practices, and the dynamics thereof. The AI/ML trustworthiness field is work-in-progress as it pertains to: techniques, tools, and regulatory provisions. In light of this ongoing evolution, we plan to update the material to keep up with collective progress made by academia, industry, government, and public interest technology/policy initiatives.	
			</p>
		</div>

	
	<!-- Table -->
	<h4>Schedule and Materials</h4>
	<p style="color:#00274C>Take the below schedule as tentative, depending on progress it will be updated as the semester advances.</p>
	
		<table class="table table-bordered"style="background-color:#00274C;color:#FFCB05">
            <thead class="table-primary">
                <tr id="section1">
                    <th>Week</th>
                    <th>Topic</th>
		    <th>Slides</th>	
                    <th>Resources/Reading</th>
                </tr>
            </thead>
            <tbody>
                <!-- Row 1 -->
                <tr>
                    <td>1</td>
                    <td>A Crash Course on Deep Neural Networks</td>
		    <td><a href= "#">slides</a></td>
                    <td>
			<ol>
			  		<li><a href="" target="_blank">Reading 1</a></li>
			  		<li><a href="" target="_blank">Reading 2</a></li>
			  		<li><a href="" target="_blank">Reading 3</a></li>
			  	</ol>
		    </td>
                </tr>
                
                <!-- Row 2 -->
                <tr>
                    <td>2</td>
                    <td>The Two Faces of ML Progress</td>
		     <td><a href= "#">slides</a></td>
                    <td>
			<ol>
			  		<li><a href="https://www.science.org/doi/10.1126/science.abi5052" target="_blank">Birhanu Eshete, Making Machine Learning Trustworthy</a></li>
			  		<li><a href="https://krvarshney.github.io/pubs/Varshney_xrds2019.pdf" target = "_blank">Kush R. Varshney, Trustworthy Machine Learning and Artificial Intelligence</a></li>
			  		<li><a href="" target="_blank">Reading 3</a></li>
			  	</ol>
		    </td>                </tr>

                <!-- Row 3 -->
                <tr>
                    <td>3</td>
                    <td>ML Attack Surface</td>
		    <td><a href= "#">slides</a></td>
                    <td>
			<ol>
			  		<li><a href="https://oaklandsok.github.io/papers/papernot2018.pdf" target="_blank"> Papernot et al., SoK: Security and Privacy in Machine Learning</a></li>
			  		<li><a href="https://people.eecs.berkeley.edu/~adj/publications/paper-files/SecML-MLJ2010.pdf" target="_blank">Barreno et al., The security of machine learning</a></li>
			  		<li><a href="https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-2e2023.ipd.pdf" target="_blank">Adversarial Machine Learning: A Taxonomy and Terminology of Attacks and Mitigations</a></li>
			  	</ol>
		    </td>                </tr>


                <!-- Row 4 -->
                <tr>
                    <td>4</td>
                    <td>Training Data Poisoning</td>
                    <td><a href= "#">slides</a></td>
                    <td>
			<ol>
			  		<li><a href="" target="_blank">Reading 1</a></li>
			  		<li><a href="" target="_blank">Reading 2</a></li>
			  		<li><a href="" target="_blank">Reading 3</a></li>
			  	</ol>
		    </td>                </tr>

                <!-- Row 5 -->
                <tr>
                    <td>5</td>
                    <td>Adversarial Examples</td>
                    <td><a href= "#">slides</a></td>
                    <td>
			<ol>
			  		<li><a href="https://arxiv.org/pdf/1312.6199.pdf" target="_blank">Szegedy et al., Intriguing properties of neural networks</a></li>
			  		<li><a href="https://arxiv.org/pdf/1602.02697.pdf" target="_blank">Papernot et al., Practical Black-Box Attacks against Machine Learning</a></li>
			  		<li><a href="https://arxiv.org/pdf/1412.6572.pdf" target="_blank">Goodfellow et al., Explaining and Harnessing Adversarial Examples</a></li>
					<li><a href="https://arxiv.org/pdf/2108.13952.pdf" target="_blank">Amich and Eshete, Morphence: Moving Target Defense Against Adversarial Examples</a></li>

			  	</ol>
		    </td>                </tr>

                <!-- Row 6 -->
                <tr>
                    <td>6</td>
                    <td>Membership Inference</td>
                    <td><a href= "#">slides</a></td>
                    <td>
			<ol>
			  		<li><a href="https://arxiv.org/pdf/1610.05820.pdf" target="_blank">Shokri et al., Membership Inference Attacks against Machine Learning Models</a></li>
			  		<li><a href="https://arxiv.org/pdf/1610.05755.pdf" target="_blank">Papernot et al., Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data</a></li>
			  		<li><a href="https://arxiv.org/pdf/1607.00133.pdf" target="_blank">Abadi et al., Deep Learning with Differential Privacy</a></li>
					<li><a href="https://arxiv.org/pdf/2112.12998.pdf" target="_blank">Jarin and Eshete., DP-UTIL: Comprehensive Utility Analysis of Differential Privacy in Machine Learning</a></li>
					<li><a href="https://petsymposium.org/popets/2023/popets-2023-0024.pdf" target="_blank">Jarin and Eshete, MIAShield: Defending Membership Inference Attacks via Preemptive Exclusion of Members</a></li>

			  	</ol>
		    </td>                </tr>

                <!-- Row 7 -->
                <tr>
                    <td>7</td>
                    <td>Model Stealing</td>
                    <td><a href= "#">slides</a></td>
                    <td>
			<ol>
			  		<li><a href="https://www.usenix.org/system/files/conference/usenixsecurity16/sec16_paper_tramer.pdf" target="_blank">Tramer et al., Stealing Machine Learning Models via Prediction APIs</a></li>
			  		<li><a href="" target="_blank">Reading 2</a></li>
			  		<li><a href="https://arxiv.org/pdf/2002.12200.pdf" target="_blank">Jia et al., Entangled Watermarks as a Defense against Model Extraction</a></li>
			  	</ol>
		    </td>                </tr>

                <!-- Row 8 -->
                <tr>
                    <td>8</td>
                    <td>Transparency and Interpretability</td>
                    <td><a href= "#">slides</a></td>
                    <td>
			<ol>
			  		<li><a href="" target="_blank">Reading 1</a></li>
			  		<li><a href="" target="_blank">Reading 2</a></li>
			  		<li><a href="" target="_blank">Reading 3</a></li>
			  	</ol>
		    </td>                </tr>

                <!-- Row 9 -->
                <tr>
                    <td>9</td>
                    <td>Fairness</td>
                    <td><a href= "#">slides</a></td>
                    <td>
			<ol>
			  		<li><a href="https://arxiv.org/pdf/1104.3913.pdf" target="_blank">Dwork et al., Fairness Through Awareness</a></li>
					<li><a href="https://www.cs.toronto.edu/~toni/Papers/icml-final.pdf" target="_blank">Zemel et al., Learning Fair Representations</a></li>
			  		<li><a href="https://arxiv.org/pdf/1610.02413.pdf" target="_blank">Hardt et al., Equality of Opportunity in Supervised Learning</a></li>
			  	</ol>
		    </td>                </tr>


		     <!-- Row 10 -->
                <tr>
                    <td>10</td>
                    <td>Holistic Trustworthiness Considerations and Open Issues </td>
                    <td><a href= "#">slides</a></td>
                    <td>
			<ol>
			  		<li><a href="" target="_blank">Reading 1</a></li>
			  		<li><a href="" target="_blank">Reading 2</a></li>
			  		<li><a href="" target="_blank">Reading 3</a></li>
			  	</ol>
		    </td>                </tr>
                
                
            </tbody>
        </table>
    
<p>&copy; Birhanu Eshete 2024 </p>
</div>

    <!-- Bootstrap JS and Popper.js -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>

</body>
</html>
